{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29823a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c3c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_second(x):\n",
    "    return x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4684f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(x):\n",
    "    if np.sum(x**2)>0:\n",
    "        normed_x = x / np.sqrt(np.sum(x**2))\n",
    "    else:\n",
    "        normed_x = x.copy()\n",
    "    return normed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34273c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(mat):\n",
    "    sq_mat = mat**2\n",
    "    sum_sq = np.sum(sq_mat, axis=1)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    mag[mag == 0] = 1\n",
    "    normed_mat = np.transpose((np.transpose(mat) / mag))\n",
    "    return normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f16c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(word, word_dic, word_matrix, n=20):\n",
    "    phon_sim = word_matrix[word_dic[word]] @ np.transpose(word_matrix)\n",
    "    word_list = [i for i in word_dic]\n",
    "    output = list(zip(word_list, phon_sim))\n",
    "    output.sort(key = sel_second, reverse=True)\n",
    "    return output[1:(n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08a62d",
   "metadata": {},
   "source": [
    "The txt files for the three types of vectors can be found at our OSF page at https://osf.io/6mys9/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e5a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LSA semantic vectors\n",
    "with open('LSA semantic vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def13d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "sem_list = []\n",
    "sem_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    sem_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    sem_matrix.append(word_vec)\n",
    "sem_matrix = np.array(sem_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2984f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "sem_dic = {}\n",
    "for i in range(0, len(sem_list)):\n",
    "    sem_dic[sem_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597965ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load phonology vectors\n",
    "with open('Phonology vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22b787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "phon_list = []\n",
    "phon_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    phon_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    phon_matrix.append(word_vec)\n",
    "phon_matrix = np.array(phon_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e7bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "phon_dic = {}\n",
    "for i in range(0, len(phon_list)):\n",
    "    phon_dic[phon_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3571f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load orthography vectors\n",
    "with open('Orthography vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5b45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "ortho_list = []\n",
    "ortho_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    ortho_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    ortho_matrix.append(word_vec)\n",
    "ortho_matrix = np.array(ortho_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff0dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "ortho_dic = {}\n",
    "for i in range(0, len(ortho_list)):\n",
    "    ortho_dic[ortho_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3b252dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize matrices\n",
    "sem_matrix = normalize_matrix(sem_matrix)\n",
    "phon_matrix = normalize_matrix(phon_matrix)\n",
    "ortho_matrix = normalize_matrix(ortho_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08804df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find words common to three matrices\n",
    "combined_words = [i for i in sem_dic if i in phon_dic and i in ortho_dic and i in sem_dic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb3597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = {}\n",
    "for i in range(0, len(combined_words)):\n",
    "    word_dic[combined_words[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a23f4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = []\n",
    "for i in combined_words:\n",
    "    sem_vec = sem_matrix[sem_dic[i]].copy()\n",
    "    phon_vec = phon_matrix[phon_dic[i]].copy()\n",
    "    ortho_vec = ortho_matrix[ortho_dic[i]].copy()\n",
    "    word_matrix.append(np.concatenate((sem_vec, phon_vec, ortho_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee9ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute semantic vector of 'teething' in for full 'teethe' vector\n",
    "teethe_sem = sem_matrix[sem_dic['teething']]\n",
    "teethe_phon = phon_matrix[phon_dic['teethe']]\n",
    "teethe_ortho = ortho_matrix[ortho_dic['teethe']]\n",
    "teethe_vec = np.concatenate((teethe_sem, teethe_phon, teethe_ortho))\n",
    "word_matrix.append(teethe_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e59fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'teethe' to word dictionary\n",
    "word_dic['teethe'] = len(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f148e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize full matrix\n",
    "word_matrix = np.array(word_matrix)\n",
    "word_matrix = normalize_matrix(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5afb5f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snows', 0.69701505),\n",
       " ('snowy', 0.6373938),\n",
       " ('snowman', 0.6143952),\n",
       " ('snowed', 0.6124683),\n",
       " ('snowstorm', 0.5907043),\n",
       " ('snowfall', 0.5447408),\n",
       " ('snowflakes', 0.5377385),\n",
       " ('snowshoe', 0.52864766),\n",
       " ('snowball', 0.5247484),\n",
       " ('snowstorms', 0.50575155)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_neighbors('snow', word_dic, word_matrix, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5efd1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors_by_space(word, word_dic, word_matrix, space='sem', n=30):\n",
    "    if space == 'sem':\n",
    "        target_word = word_matrix[word_dic[word]][:300]\n",
    "        word_space = word_matrix[:, 0:300]\n",
    "    elif space == 'phon':\n",
    "        target_word = word_matrix[word_dic[word]][300:600]\n",
    "        word_space = word_matrix[:, 300:600]\n",
    "    elif space == 'ortho':\n",
    "        target_word = word_matrix[word_dic[word]][600:900]\n",
    "        word_space = word_matrix[:, 600:900]\n",
    "    elif space == 'all':\n",
    "        target_word = normalize(word_matrix[word_dic[word]].copy())\n",
    "        word_space = word_matrix\n",
    "    word_space = normalize_matrix(word_space)\n",
    "    word_list = [i for i in word_dic]\n",
    "    cos_sim = normalize_vec(target_word) @ np.transpose(word_space)\n",
    "    cos_sim[word_dic[word]] = 0\n",
    "    cos_list = list(zip(word_list, cos_sim))\n",
    "    cos_list.sort(key=sel_second, reverse=True)\n",
    "    return cos_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c93d9efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('catch', 0.7452618),\n",
       " ('catching', 0.6926295),\n",
       " ('threw', 0.67456704),\n",
       " ('leaped', 0.6655893),\n",
       " ('wildly', 0.6563325),\n",
       " ('tossed', 0.655919),\n",
       " ('suddenly', 0.6557272),\n",
       " ('grabbed', 0.6513512),\n",
       " ('quick', 0.64505243),\n",
       " ('out', 0.6365896)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_neighbors_by_space('caught', word_dic, word_matrix, space='sem', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52844014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cot', 0.9999999),\n",
       " ('cots', 0.84441763),\n",
       " ('cotton', 0.7897006),\n",
       " ('scott', 0.78440845),\n",
       " ('scot', 0.78440845),\n",
       " ('cotter', 0.7636759),\n",
       " ('tock', 0.7596828),\n",
       " ('tot', 0.74973047),\n",
       " ('cotta', 0.7444337),\n",
       " ('got', 0.7368597)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_neighbors_by_space('caught', word_dic, word_matrix, space='phon', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df3d1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aught', 0.86199254),\n",
       " ('taught', 0.7695395),\n",
       " ('mcnaught', 0.765159),\n",
       " ('naught', 0.7614912),\n",
       " ('haughty', 0.68023616),\n",
       " ('fraught', 0.6767743),\n",
       " ('draught', 0.67655665),\n",
       " ('naughty', 0.6681263),\n",
       " ('mcgaugh', 0.6593457),\n",
       " ('waugh', 0.6241391)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_neighbors_by_space('caught', word_dic, word_matrix, space='ortho', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756ca5e",
   "metadata": {},
   "source": [
    "MINERVA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16e8bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_memory(word_list):\n",
    "    mem_matrix = []\n",
    "    for i in word_list:\n",
    "        mem_matrix.append(word_matrix[word_dic[i]])\n",
    "    mem_matrix = np.array(mem_matrix)\n",
    "    return mem_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37be609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_intensity(probes, memory, tau=3):\n",
    "    normed_memory = normalize_matrix(memory)\n",
    "    similarities = probes @ np.transpose(normed_memory)\n",
    "    if tau == 2:\n",
    "        activations = similarities*(abs(similarities))\n",
    "    if tau == 4:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    else:\n",
    "        activations = similarities**tau\n",
    "    activations = np.sum(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c55c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def r_sq(x, y):\n",
    "    cor = scipy.stats.pearsonr(x, y)\n",
    "    return cor[0]**2\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sum((x - y)**2) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dc06a",
   "metadata": {},
   "source": [
    "Simulation Exp 1 -- Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046e8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('Stimuli_Semantic_False_Memory.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f, delimiter=',')\n",
    "    for i in csvreader:\n",
    "        df.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "374010b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute American spellings\n",
    "df[5][6] = 'organization'\n",
    "df[20][3] = 'meter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29bc20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df)):\n",
    "    df[i] = [x.lower() for x in df[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb69daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df[1:11]\n",
    "df_b = df[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d5d6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up list A for Simulation\n",
    "list_old = []\n",
    "crit_lure = []\n",
    "list_new = []\n",
    "crit_new = []\n",
    "for i in df[1:11]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_lure.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_old.append(cl)\n",
    "for i in df[11:]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_new.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_new.append(cl)\n",
    "all_items_a = list_old + crit_lure + list_new + crit_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "762559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up list B for Simulation\n",
    "list_old = []\n",
    "crit_lure = []\n",
    "list_new = []\n",
    "crit_new = []\n",
    "for i in df[11:]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_lure.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_old.append(cl)\n",
    "for i in df[1:11]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_new.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_new.append(cl)\n",
    "all_items_b = list_old + crit_lure + list_new + crit_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41056571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical means\n",
    "emp_means = np.array([0.684375, 0.44166667, 0.20069444, 0.23333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87876b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemtype: ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure']\n",
      "means:  [0.6605     0.4088     0.22873333 0.2558    ]\n",
      "SDs:  [0.0341862  0.10456335 0.05013003 0.0530016 ]\n",
      "R-squared fit:  0.9925707963827805\n",
      "Empirical means:  [0.684375   0.44166667 0.20069444 0.23333333]\n"
     ]
    }
   ],
   "source": [
    "l = .75   # Learning rate\n",
    "t = 1      # Retrieval exponent\n",
    "p_old = 43   # Percentage of items to be deemed old\n",
    "sim_list = []\n",
    "for s in range(0, 1000):\n",
    "    if s < 500:\n",
    "        stims = all_items_a.copy()\n",
    "    else:\n",
    "        stims = all_items_b.copy()\n",
    "    memory = make_memory(stims[0:60])\n",
    "    memory *= np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "    probes = make_memory(stims)\n",
    "    familiarities = echo_intensity(probes, memory, tau=t)\n",
    "    criterion = np.percentile(familiarities, 100-p_old)\n",
    "    list_rel_hits = np.sum(familiarities[:60] > criterion) / 60\n",
    "    crit_rel_hits = np.sum(familiarities[60:70] > criterion) / 10\n",
    "    list_new_hits = np.sum(familiarities[70:130] > criterion) / 60\n",
    "    crit_new_hits = np.sum(familiarities[130:140] > criterion) / 10\n",
    "    sim_list.append([list_rel_hits, crit_rel_hits, list_new_hits, crit_new_hits])\n",
    "means = np.mean(sim_list, axis=0)\n",
    "sds = np.std(sim_list, axis=0, ddof=1)\n",
    "print('itemtype:', ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure'])\n",
    "print('means: ', means)\n",
    "print('SDs: ', sds)\n",
    "print('R-squared fit: ', r_sq(emp_means, means))\n",
    "print('Empirical means: ', emp_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18f15f",
   "metadata": {},
   "source": [
    "Simulation Exp 2 -- Phonology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99d7bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stimuli\n",
    "df = []\n",
    "with open('Stimuli_Phonological_False_Memory.csv') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        df.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9439c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af048850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "beabec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cade377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up list for simulation\n",
    "df_test_list_a = []\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_a.append(i[0])\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_a.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f69dec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up list for simulation\n",
    "df_test_list_b = []\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_b.append(i[0])\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_b.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0af5ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [0.682     , 0.578     , 0.35033333, 0.4]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb509d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemtype: ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure']\n",
      "means:  [0.67465    0.6283     0.34246667 0.369     ]\n",
      "SDs:  [0.04748589 0.15333826 0.04833939 0.12827882]\n",
      "R-squared fit:  0.9668482932684399\n",
      "Empirical means:  [0.682, 0.578, 0.35033333, 0.4]\n"
     ]
    }
   ],
   "source": [
    "l = 0.05    # Learning rate\n",
    "t = 1       # Retrieval exponent\n",
    "p_old = 51   # Percentage of old items\n",
    "sim_list = []\n",
    "for s in range(0, 1000):\n",
    "    if s < 500:\n",
    "        stims = df_test_list_a.copy()\n",
    "    else:\n",
    "        stims = df_test_list_b.copy()\n",
    "    memory = make_memory(stims[0:60])\n",
    "    memory = memory * np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "    probes = make_memory(stims)\n",
    "    familiarities = echo_intensity(probes, memory, tau=t)\n",
    "    criterion = np.percentile(familiarities, 100-p_old)\n",
    "    old_items = familiarities[0:60]\n",
    "    crit_lure = familiarities[60:70]\n",
    "    unrel_items = familiarities[70:130]\n",
    "    uncrit_items = familiarities[130:140]\n",
    "    old_hits = np.sum(old_items > criterion) / 60\n",
    "    crit_hits = np.sum(crit_lure > criterion) / 10\n",
    "    unrel_hits = np.sum(unrel_items > criterion) / 60\n",
    "    uncrit_hits = np.sum(uncrit_items > criterion) / 10\n",
    "    sim_list.append([old_hits, crit_hits, unrel_hits, uncrit_hits])\n",
    "means = np.mean(sim_list, axis=0)\n",
    "sds = np.std(sim_list, axis=0, ddof=1)\n",
    "print('itemtype:', ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure'])\n",
    "print('means: ', means)\n",
    "print('SDs: ', sds)\n",
    "print('R-squared fit: ', r_sq(emp_data, means))\n",
    "print('Empirical means: ', emp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9432d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
