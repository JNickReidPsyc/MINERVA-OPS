{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a6030",
   "metadata": {},
   "source": [
    "For code to construct Phonology Vectors, see Parrish (2017):\n",
    "\n",
    "Parrish, A. (2017, October). Poetic sound similarity vectors using Phonetic Features [Paper presentation]. \n",
    "AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, Snowbird, Utah, USA. https://www.aaai.org/ocs/index.php/AIIDE/AIIDE17/paper/view/15879/15227\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4718b2d",
   "metadata": {},
   "source": [
    "You can download the \"Phonology vectors.txt\" from our OSF page at https://osf.io/6mys9/\n",
    "This file contains the pre-trained phonology vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f351f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load phonology vectors\n",
    "with open('Phonology vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b758b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "word_list = []\n",
    "word_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    word_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    word_matrix.append(word_vec)\n",
    "word_matrix = np.array(word_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a508db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "word_dic = {}\n",
    "for i in range(0, len(word_list)):\n",
    "    word_dic[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2e72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vec):\n",
    "    sq_vec = vec**2\n",
    "    sum_sq = np.sum(sq_vec)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    normed_vec = vec / mag\n",
    "    return normed_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc303a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(mat):\n",
    "    sq_mat = mat**2\n",
    "    sum_sq = np.sum(sq_mat, axis=1)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    mag[mag == 0] = 1\n",
    "    normed_mat = np.transpose((np.transpose(mat) / mag))\n",
    "    return normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e8bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_memory(word_list):\n",
    "    mem_matrix = []\n",
    "    for i in word_list:\n",
    "        mem_matrix.append(word_matrix[word_dic[i]])\n",
    "    mem_matrix = np.array(mem_matrix)\n",
    "    return mem_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37be609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_intensity(probes, memory, tau=3):\n",
    "    normed_memory = normalize_matrix(memory)\n",
    "    similarities = probes @ np.transpose(normed_memory)\n",
    "    if tau == 2:\n",
    "        activations = similarities*(abs(similarities))\n",
    "    if tau == 4:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    else:\n",
    "        activations = similarities**tau\n",
    "    activations = np.sum(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef96fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def r_sq(x, y):\n",
    "    cor = scipy.stats.pearsonr(x, y)\n",
    "    return cor[0]**2\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sum((x - y)**2) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c1d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('Stimuli_Phonological_False_Memory.csv') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        df.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52007cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a1f5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c4cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f671df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list a\n",
    "df_test_list_a = []\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_a.append(i[0])\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_a.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc387199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list b\n",
    "df_test_list_b = []\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_b.append(i[0])\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_b.append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf4109",
   "metadata": {},
   "source": [
    "Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8997602",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [0.682     , 0.578     , 0.35033333, 0.4]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69580e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemtype: ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure']\n",
      "means:  [0.65485 0.5438  0.38105 0.3408 ]\n",
      "SDs:  [0.04876497 0.14806581 0.05295261 0.13643304]\n",
      "R-sq fit: 0.9400511174210896\n",
      "Empirical means:  [0.682, 0.578, 0.35033333, 0.4]\n"
     ]
    }
   ],
   "source": [
    "l = 0.03\n",
    "t = 3\n",
    "p_old = 51\n",
    "sim_list = []\n",
    "for s in range(0, 1000):\n",
    "    if s < 500:\n",
    "        stims = df_test_list_a.copy()\n",
    "    else:\n",
    "        stims = df_test_list_b.copy()\n",
    "    memory = make_memory(stims[0:60])\n",
    "    memory = memory * np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "    probes = make_memory(stims)\n",
    "    familiarities = echo_intensity(probes, memory, tau=t)\n",
    "    criterion = np.percentile(familiarities, 100-p_old)\n",
    "    old_items = familiarities[0:60]\n",
    "    crit_lure = familiarities[60:70]\n",
    "    unrel_items = familiarities[70:130]\n",
    "    uncrit_items = familiarities[130:140]\n",
    "    old_hits = np.sum(old_items > criterion) / 60\n",
    "    crit_hits = np.sum(crit_lure > criterion) / 10\n",
    "    unrel_hits = np.sum(unrel_items > criterion) / 60\n",
    "    uncrit_hits = np.sum(uncrit_items > criterion) / 10\n",
    "    sim_list.append([old_hits, crit_hits, unrel_hits, uncrit_hits])\n",
    "means = np.mean(sim_list, axis=0)\n",
    "sds = np.std(sim_list, axis=0, ddof=1)\n",
    "print('itemtype:', ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure'])\n",
    "print('means: ', means)\n",
    "print('SDs: ', sds)\n",
    "print('R-sq fit:', r_sq(emp_data, means))\n",
    "print('Empirical means: ', emp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to find best parameters\n",
    "para_list = []\n",
    "t = 3\n",
    "p_old = 51\n",
    "for L in range(1, 11, 1):\n",
    "    l = L / 100\n",
    "    sim_list = []\n",
    "    for s in range(0, 1000):\n",
    "        if s < 500:\n",
    "            stims = df_test_list_a.copy()\n",
    "        else:\n",
    "            stims = df_test_list_b.copy()\n",
    "        memory = make_memory(stims[0:60])\n",
    "        memory = memory * np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "        probes = make_memory(stims)\n",
    "        familiarities = echo_intensity(probes, memory, tau=t)\n",
    "        criterion = np.percentile(familiarities, 100-p_old)\n",
    "        old_items = familiarities[0:60]\n",
    "        crit_lure = familiarities[60:70]\n",
    "        unrel_items = familiarities[70:130]\n",
    "        uncrit_items = familiarities[130:140]\n",
    "        old_hits = np.sum(old_items > criterion) / 60\n",
    "        crit_hits = np.sum(crit_lure > criterion) / 10\n",
    "        unrel_hits = np.sum(unrel_items > criterion) / 60\n",
    "        uncrit_hits = np.sum(uncrit_items > criterion) / 10\n",
    "        sim_list.append([old_hits, crit_hits, unrel_hits, uncrit_hits])\n",
    "    means = np.mean(sim_list, axis=0)\n",
    "    sds = np.std(sim_list, axis=0, ddof=1)\n",
    "    r_fit = r_sq(means, emp_data)\n",
    "    mse_fit = mse(means, emp_data)\n",
    "    para_list.append([l, r_fit, mse_fit, means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d59b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_third(x):\n",
    "    return x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "369e6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by lowest mean square error\n",
    "para_list.sort(key=sel_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a8830ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03,\n",
       "  0.9501042520399086,\n",
       "  0.001382640181527799,\n",
       "  array([0.65961667, 0.5428    , 0.37591667, 0.344     ])],\n",
       " [0.04,\n",
       "  0.9575428215657519,\n",
       "  0.0023796490101944032,\n",
       "  array([0.69981667, 0.5549    , 0.33978333, 0.3075    ])],\n",
       " [0.02,\n",
       "  0.9245480924417957,\n",
       "  0.0032959369260277325,\n",
       "  array([0.60781667, 0.5301    , 0.42261667, 0.3873    ])],\n",
       " [0.05,\n",
       "  0.9665573662011808,\n",
       "  0.005730520192333293,\n",
       "  array([0.7435    , 0.573     , 0.29906667, 0.2716    ])],\n",
       " [0.01,\n",
       "  0.8454810330339829,\n",
       "  0.008704389223583199,\n",
       "  array([0.55805   , 0.5109    , 0.46781667, 0.4339    ])],\n",
       " [0.06,\n",
       "  0.9751981214647255,\n",
       "  0.008866749103194512,\n",
       "  array([0.77253333, 0.5857    , 0.27058333, 0.2556    ])],\n",
       " [0.07,\n",
       "  0.9730218181730962,\n",
       "  0.013016062053333593,\n",
       "  array([0.79906667, 0.5792    , 0.249     , 0.2324    ])],\n",
       " [0.08,\n",
       "  0.9760931513520864,\n",
       "  0.01792487159652829,\n",
       "  array([0.82428333, 0.5932    , 0.22491667, 0.2116    ])],\n",
       " [0.09,\n",
       "  0.976279884312816,\n",
       "  0.022176947887917325,\n",
       "  array([0.8432    , 0.5975    , 0.20808333, 0.1948    ])],\n",
       " [0.1,\n",
       "  0.9773467399865171,\n",
       "  0.026009256962000606,\n",
       "  array([0.85753333, 0.6085    , 0.1942    , 0.1811    ])]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9fc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
