{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa43d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a6030",
   "metadata": {},
   "source": [
    "For code to construct Phonology Vectors, see Parrish (2017):\n",
    "\n",
    "Parrish, A. (2017, October). Poetic sound similarity vectors using Phonetic Features [Paper presentation]. \n",
    "AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, Snowbird, Utah, USA. https://www.aaai.org/ocs/index.php/AIIDE/AIIDE17/paper/view/15879/15227\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4718b2d",
   "metadata": {},
   "source": [
    "You can download the \"Phonology vectors.txt\" from our OSF page at https://osf.io/6mys9/\n",
    "This file contains the pre-trained phonology vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f351f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load phonology vectors\n",
    "with open('Phonology vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b758b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "word_list = []\n",
    "word_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    word_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    word_matrix.append(word_vec)\n",
    "word_matrix = np.array(word_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a508db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "word_dic = {}\n",
    "for i in range(0, len(word_list)):\n",
    "    word_dic[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2e72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vec):\n",
    "    sq_vec = vec**2\n",
    "    sum_sq = np.sum(sq_vec)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    normed_vec = vec / mag\n",
    "    return normed_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc303a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(mat):\n",
    "    sq_mat = mat**2\n",
    "    sum_sq = np.sum(sq_mat, axis=1)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    mag[mag == 0] = 1\n",
    "    normed_mat = np.transpose((np.transpose(mat) / mag))\n",
    "    return normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e8bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_memory(word_list):\n",
    "    mem_matrix = []\n",
    "    for i in word_list:\n",
    "        mem_matrix.append(word_matrix[word_dic[i]])\n",
    "    mem_matrix = np.array(mem_matrix)\n",
    "    return mem_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37be609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_intensity(probes, memory, tau=3):\n",
    "    normed_memory = normalize_matrix(memory)\n",
    "    similarities = probes @ np.transpose(normed_memory)\n",
    "    if tau == 2:\n",
    "        activations = similarities*(abs(similarities))\n",
    "    if tau == 4:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    else:\n",
    "        activations = similarities**tau\n",
    "    activations = np.sum(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef96fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def r_sq(x, y):\n",
    "    cor = scipy.stats.pearsonr(x, y)\n",
    "    return cor[0]**2\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sum((x - y)**2) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c1d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('Stimuli_Phonological_False_Memory.csv') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        df.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52007cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a1f5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c4cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f671df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list a\n",
    "df_test_list_a = []\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_a.append(i[0])\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_a.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_a.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc387199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list b\n",
    "df_test_list_b = []\n",
    "for i in df_b:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_b:\n",
    "    df_test_list_b.append(i[0])\n",
    "for i in df_a:\n",
    "    for x in i[1:]:\n",
    "        df_test_list_b.append(x)\n",
    "for i in df_a:\n",
    "    df_test_list_b.append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf4109",
   "metadata": {},
   "source": [
    "Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8997602",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [0.682     , 0.578     , 0.35033333, 0.4]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69580e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemtype: ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure']\n",
      "means:  [0.6565     0.5434     0.37888333 0.3443    ]\n",
      "SDs:  [0.0501218  0.15052056 0.05470555 0.13640935]\n",
      "R-sq fit: 0.9468866119795487\n"
     ]
    }
   ],
   "source": [
    "l = 0.03\n",
    "t = 3\n",
    "p_old = 51\n",
    "sim_list = []\n",
    "for s in range(0, 1000):\n",
    "    if s < 500:\n",
    "        stims = df_test_list_a.copy()\n",
    "    else:\n",
    "        stims = df_test_list_b.copy()\n",
    "    memory = make_memory(stims[0:60])\n",
    "    memory = memory * np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "    probes = make_memory(stims)\n",
    "    familiarities = echo_intensity(probes, memory, tau=t)\n",
    "    criterion = np.percentile(familiarities, 100-p_old)\n",
    "    old_items = familiarities[0:60]\n",
    "    crit_lure = familiarities[60:70]\n",
    "    unrel_items = familiarities[70:130]\n",
    "    uncrit_items = familiarities[130:140]\n",
    "    old_hits = np.sum(old_items > criterion) / 60\n",
    "    crit_hits = np.sum(crit_lure > criterion) / 10\n",
    "    unrel_hits = np.sum(unrel_items > criterion) / 60\n",
    "    uncrit_hits = np.sum(uncrit_items > criterion) / 10\n",
    "    sim_list.append([old_hits, crit_hits, unrel_hits, uncrit_hits])\n",
    "means = np.mean(sim_list, axis=0)\n",
    "sds = np.std(sim_list, axis=0)\n",
    "print('itemtype:', ['target list/list item', 'target list/critical lure', 'foil list/list item', 'foil list/critical lure'])\n",
    "print('means: ', means)\n",
    "print('SDs: ', sds)\n",
    "print('R-sq fit:', r_sq(emp_data, means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20d836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to find best parameters\n",
    "para_list = []\n",
    "t = 3\n",
    "p_old = 51\n",
    "for L in range(1, 11, 1):\n",
    "    l = L / 100\n",
    "    sim_list = []\n",
    "    for s in range(0, 1000):\n",
    "        if s < 500:\n",
    "            stims = df_test_list_a.copy()\n",
    "        else:\n",
    "            stims = df_test_list_b.copy()\n",
    "        memory = make_memory(stims[0:60])\n",
    "        memory = memory * np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "        probes = make_memory(stims)\n",
    "        familiarities = echo_intensity(probes, memory, tau=t)\n",
    "        criterion = np.percentile(familiarities, 100-p_old)\n",
    "        old_items = familiarities[0:60]\n",
    "        crit_lure = familiarities[60:70]\n",
    "        unrel_items = familiarities[70:130]\n",
    "        uncrit_items = familiarities[130:140]\n",
    "        old_hits = np.sum(old_items > criterion) / 60\n",
    "        crit_hits = np.sum(crit_lure > criterion) / 10\n",
    "        unrel_hits = np.sum(unrel_items > criterion) / 60\n",
    "        uncrit_hits = np.sum(uncrit_items > criterion) / 10\n",
    "        sim_list.append([old_hits, crit_hits, unrel_hits, uncrit_hits])\n",
    "    means = np.mean(sim_list, axis=0)\n",
    "    sds = np.std(sim_list, axis=0)\n",
    "    r_fit = r_sq(means, emp_data)\n",
    "    mse_fit = mse(means, emp_data)\n",
    "    para_list.append([l, r_fit, mse_fit, means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d59b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_third(x):\n",
    "    return x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "369e6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by lowest mean square error\n",
    "para_list.sort(key=sel_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a8830ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03,\n",
       "  0.946843158830262,\n",
       "  0.0013847595579166757,\n",
       "  array([0.65783333, 0.5526    , 0.37675   , 0.3399    ])],\n",
       " [0.04,\n",
       "  0.951867764278088,\n",
       "  0.0028158469944443527,\n",
       "  array([0.70201667, 0.5574    , 0.33866667, 0.2985    ])],\n",
       " [0.02,\n",
       "  0.9271806276941146,\n",
       "  0.003411603804527679,\n",
       "  array([0.60466667, 0.5327    , 0.42471667, 0.391     ])],\n",
       " [0.05,\n",
       "  0.9671184587772628,\n",
       "  0.005540265819444397,\n",
       "  array([0.74121667, 0.5778    , 0.30033333, 0.2729    ])],\n",
       " [0.01,\n",
       "  0.9069826507274845,\n",
       "  0.008684409706944321,\n",
       "  array([0.55681667, 0.5175    , 0.46616667, 0.4446    ])],\n",
       " [0.06,\n",
       "  0.9726962520120538,\n",
       "  0.00929376222819444,\n",
       "  array([0.77328333, 0.5865    , 0.27058333, 0.2503    ])],\n",
       " [0.07,\n",
       "  0.9747983139237092,\n",
       "  0.01313021767669463,\n",
       "  array([0.79843333, 0.5897    , 0.24801667, 0.2316    ])],\n",
       " [0.08,\n",
       "  0.9741979915012103,\n",
       "  0.018019124307611618,\n",
       "  array([0.82328333, 0.5922    , 0.22656667, 0.2087    ])],\n",
       " [0.09,\n",
       "  0.9780627975772402,\n",
       "  0.021874143165583993,\n",
       "  array([0.8417    , 0.6043    , 0.20801667, 0.1974    ])],\n",
       " [0.1,\n",
       "  0.9774334983940173,\n",
       "  0.02532536286691709,\n",
       "  array([0.8568    , 0.6013    , 0.19548333, 0.185     ])]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
