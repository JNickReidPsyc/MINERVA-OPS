{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b30b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746184f",
   "metadata": {},
   "source": [
    "You can download the \"LSA semantic vectors.txt\" from our OSF page at https://osf.io/6mys9/ This file contains the pre-trained LSA vectors trained on TASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01338f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load LSA semantic vectors\n",
    "with open('LSA semantic vectors.txt', 'r') as f:\n",
    "    df = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82478ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw txt file into word list and matrix\n",
    "df = df.split('\\n')\n",
    "df = [i.strip() for i in df]\n",
    "word_list = []\n",
    "word_matrix = []\n",
    "for i in df:\n",
    "    array = i.split(' ')\n",
    "    word_list.append(array[0])\n",
    "    word_vec = []\n",
    "    for x in array[1:]:\n",
    "        word_vec.append(float(x))\n",
    "    word_matrix.append(word_vec)\n",
    "word_matrix = np.array(word_matrix, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe8106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word dictionary to index rows of matrix by word\n",
    "word_dic = {}\n",
    "for i in range(0, len(word_list)):\n",
    "    word_dic[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44bea324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def r_sq(x, y):\n",
    "    cor = scipy.stats.pearsonr(x, y)\n",
    "    return cor[0]**2\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.sum((x - y)**2) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfeda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vec(vec):\n",
    "    sq_vec = vec**2\n",
    "    sum_sq = np.sum(sq_vec)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    normed_vec = vec / mag\n",
    "    return normed_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2a02a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(mat):\n",
    "    sq_mat = mat**2\n",
    "    sum_sq = np.sum(sq_mat, axis=1)\n",
    "    mag = np.sqrt(sum_sq)\n",
    "    mag[mag == 0] = 1\n",
    "    normed_mat = np.transpose((np.transpose(mat) / mag))\n",
    "    return normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531baa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_memory(word_list):\n",
    "    mem_matrix = []\n",
    "    for i in word_list:\n",
    "        mem_matrix.append(word_matrix[word_dic[i]])\n",
    "    mem_matrix = np.array(mem_matrix)\n",
    "    return mem_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "093cf07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_intensity(probes, memory, tau=3):\n",
    "    normed_memory = normalize_matrix(memory)\n",
    "    similarities = probes @ np.transpose(normed_memory)\n",
    "    if tau == 2:\n",
    "        activations = similarities*(abs(similarities))\n",
    "    if tau == 4:\n",
    "        activations = similarities*(abs(similarities))*similarities*(abs(similarities))\n",
    "    else:\n",
    "        activations = similarities**tau\n",
    "    activations = np.sum(activations, axis=1)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517ae3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('Stimuli_Semantic_False_Memory.csv', 'r') as f:\n",
    "    csvreader = csv.reader(f, delimiter=',')\n",
    "    for i in csvreader:\n",
    "        df.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665b71d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['List',\n",
       "  'Target Word',\n",
       "  'Stimulus 01',\n",
       "  'Stimulus 02',\n",
       "  'Stimulus 03',\n",
       "  'Stimulus 04',\n",
       "  'Stimulus 05',\n",
       "  'Stimulus 06'],\n",
       " ['1', 'BASS', 'guitar', 'treble', 'drum', 'fish', 'music ', 'boom'],\n",
       " ['2', 'BRAKE', 'stop', 'pedal', 'car', 'clutch', 'accelerate ', 'speed'],\n",
       " ['3', 'CLAM', 'oyster', 'seafood', 'shell', 'chowder', 'pearl', 'mussel '],\n",
       " ['4', 'CLAMP', 'hold', 'tight', 'vise', 'chisel', 'tool', 'metal'],\n",
       " ['5', 'CLUB', 'golf', 'member', 'ball', 'dance ', 'organisation', 'house'],\n",
       " ['6', 'DOVE', 'bird', 'peace', 'white', 'beak', 'bar', 'feather'],\n",
       " ['7', 'FEAR', 'scared', 'fright', 'terror', 'anxiety', 'monster', 'snake'],\n",
       " ['8', 'PLANE', 'jet', 'air', 'fly', 'sky', 'travel', 'geometry'],\n",
       " ['9', 'PINK', 'panther', 'pretty', 'purple', 'lemonade ', 'rose', 'dress'],\n",
       " ['10', 'GIN', 'tonic', 'alcohol', 'vodka', 'drink', 'liquor', 'drunk'],\n",
       " ['11', 'FRAUD', 'fake', 'cheat', 'lie', 'crime', 'false ', 'money '],\n",
       " ['12',\n",
       "  'FILE',\n",
       "  'cabinet',\n",
       "  'paper',\n",
       "  'folder',\n",
       "  'drawer',\n",
       "  'document',\n",
       "  'misfiling'],\n",
       " ['13', 'FLASH', 'light', 'camera', 'bulb', 'bright', 'back', 'flood'],\n",
       " ['14', 'GLASS', 'window', 'crystal', 'cup', 'bottle', 'clear', 'jar'],\n",
       " ['15', 'LEAF', 'tree', 'maple', 'branch', 'flower', 'fall', 'pot'],\n",
       " ['16', 'NECK', 'throat', 'tie', 'collar', 'necklace', 'shoulder', 'long'],\n",
       " ['17', 'ROAST', 'beef', 'pork', 'cook', 'turkey ', 'oven', 'dinner'],\n",
       " ['18', 'VEST', 'suit', 'jacket', 'shirt', 'blouse', 'coat ', 'pants'],\n",
       " ['19', 'GAS', 'methane', 'station', 'energy', 'stove', 'heat', 'liquid'],\n",
       " ['20', 'YARD', 'mile', 'metre', 'inch', 'grass', 'stick', 'foot']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e061b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute American spellings\n",
    "df[5][6] = 'organization'\n",
    "df[20][3] = 'meter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14bef04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up list A\n",
    "list_old = []\n",
    "crit_lure = []\n",
    "list_new = []\n",
    "crit_new = []\n",
    "for i in df[1:11]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_lure.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_old.append(cl)\n",
    "for i in df[11:]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_new.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_new.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc7551d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_a = list_old + crit_lure + list_new + crit_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165eccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up list B\n",
    "list_old = []\n",
    "crit_lure = []\n",
    "list_new = []\n",
    "crit_new = []\n",
    "for i in df[11:]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_lure.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_old.append(cl)\n",
    "for i in df[1:11]:\n",
    "    cl = i[1].strip()\n",
    "    cl = cl.lower()\n",
    "    crit_new.append(cl)\n",
    "    for x in i[2:]:\n",
    "        cl = x.strip()\n",
    "        cl = cl.lower()\n",
    "        list_new.append(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d92fd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_b = list_old + crit_lure + list_new + crit_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c245da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize LSA matrix before simulation\n",
    "word_matrix = normalize_matrix(word_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393c4bf",
   "metadata": {},
   "source": [
    "Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79e95799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empirical means\n",
    "emp_means = np.array([0.684375, 0.44166667, 0.20069444, 0.23333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "150ba182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means:  [0.69208333 0.4457     0.19416667 0.2368    ]\n",
      "SDs:  [0.06508408 0.12418337 0.06108168 0.08582401]\n"
     ]
    }
   ],
   "source": [
    "l = 0.22 #learning rate\n",
    "t = 3      #retrieval exponent\n",
    "p_old = 43   # percentage of items to be deemed old\n",
    "sim_list = []\n",
    "for s in range(0, 1000):\n",
    "    if s < 500:\n",
    "        stims = all_items_a.copy()\n",
    "    else:\n",
    "        stims = all_items_b.copy()\n",
    "    memory = make_memory(stims[0:60])\n",
    "    memory *= np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "    probes = make_memory(stims)\n",
    "    familiarities = echo_intensity(probes, memory, tau=t)\n",
    "    criterion = np.percentile(familiarities, 100-p_old)\n",
    "    list_rel_hits = np.sum(familiarities[:60] > criterion) / 60\n",
    "    crit_rel_hits = np.sum(familiarities[60:70] > criterion) / 10\n",
    "    list_new_hits = np.sum(familiarities[70:130] > criterion) / 60\n",
    "    crit_new_hits = np.sum(familiarities[130:140] > criterion) / 10\n",
    "    sim_list.append([list_rel_hits, crit_rel_hits, list_new_hits, crit_new_hits])\n",
    "print('means: ', np.mean(sim_list, axis=0))\n",
    "print('SDs: ', np.std(sim_list, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cad49731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997159710793717"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sq(emp_means, np.mean(sim_list, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a1b578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to find best parameters\n",
    "t = 3\n",
    "p_old = 43\n",
    "para_list = []\n",
    "for L in range(1, 31):\n",
    "    l = L / 100\n",
    "    sim_list = []\n",
    "    for s in range(0, 1000):\n",
    "        if s < 500:\n",
    "            stims = all_items_a.copy()\n",
    "        else:\n",
    "            stims = all_items_b.copy()\n",
    "        memory = make_memory(stims[0:60])\n",
    "        memory *= np.random.choice([0, 1], size=(len(memory), len(memory[0])), p=[1-l, l])\n",
    "        probes = make_memory(stims)\n",
    "        familiarities = echo_intensity(probes, memory, tau=t)\n",
    "        criterion = np.percentile(familiarities, 100-p_old)\n",
    "        list_rel_hits = np.sum(familiarities[:60] > criterion) / 60\n",
    "        crit_rel_hits = np.sum(familiarities[60:70] > criterion) / 10\n",
    "        list_new_hits = np.sum(familiarities[70:130] > criterion) / 60\n",
    "        crit_new_hits = np.sum(familiarities[130:140] > criterion) / 10\n",
    "        sim_list.append([list_rel_hits, crit_rel_hits, list_new_hits, crit_new_hits])\n",
    "    sim_means = np.mean(sim_list, axis=0)\n",
    "    r_sq_sim = r_sq(emp_means, sim_means)\n",
    "    mse_sim = mse(emp_means, sim_means)\n",
    "    para_list.append([l, r_sq_sim, mse_sim, sim_means])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53f778e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_third(x):\n",
    "    return x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "321b1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list.sort(key=sel_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52104275",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
